{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "np.random.seed(0)\n",
    "x = ortho_group.rvs(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "std_list = [1.5, 2.0, 1.5, 2.5, 1.7, 2.2]\n",
    "fdf = pd.DataFrame()\n",
    "for i in range(6):\n",
    "    fdf[i] = np.random.normal(loc=0.0, scale=std_list[i], size=200000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[ 0.47669462  0.15964814 -0.47766656  0.55284255 -0.13930486  0.44053222]\n[ 0.10813329 -0.8838343   0.06718921  0.39309219  0.13141423 -0.1756091 ]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(x[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "fdf[\"group\"] = 0.9 / (1.0 + np.exp(-np.matmul(fdf.values, x[0].T) + 5.0)) +\\\n",
    "               0.1 / (1.0 + np.exp(-np.matmul(fdf.values, x[1].T) + 6.0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "fdf[\"prob\"] = np.random.uniform(low=0.0, high=1.0, size=200000)\n",
    "fdf[\"label\"] = fdf[\"group\"] > fdf[\"prob\"]\n",
    "fdf[\"label\"] = fdf[\"label\"] * 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0    193851\n1      6149\nName: label, dtype: int64"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "fdf.label.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[-3.2644018   2.16129832  1.32745473 -0.92475887 -0.84604962  0.38897996]\n [ 1.80863845  0.9712237  -1.26000599  0.38868967  0.4564715   1.39185836]\n [ 0.06042143 -1.84251117  0.79550194 -4.35093213  1.54590996  2.81069942]\n ...\n [ 2.44711711  1.14586658  2.67928386 -0.28663962  0.36477659 -0.5870959 ]\n [ 1.13644146  2.48149056 -0.24881862  0.07782861 -1.73102276 -1.25522277]\n [ 1.18222992  2.00613358  2.25983241 -1.2610145  -0.83184001 -0.58121851]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "cdf = fdf.copy()\n",
    "Y = cdf.label.values\n",
    "X = cdf.drop([\"prob\", \"label\", \"group\"], axis=1).values\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fa723d90df0>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 11
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "torch.manual_seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(Data.sampler.Sampler):\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples\n",
    "        \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "        \n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "    \n",
    "    def _get_label(self, dataset, idx):\n",
    "        return dataset[idx][1].item()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class SubNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class FrontProcessing(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrontProcessing, self).__init__()\n",
    "        self.in1 = nn.Linear(6, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.in1(x)\n",
    "        return x\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, FrontProcessing, SubNet):\n",
    "        super(Net, self).__init__()\n",
    "        self.front = FrontProcessing()\n",
    "        self.subnet1 = SubNet()\n",
    "        self.subnet2 = SubNet()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.front(x)\n",
    "        x1 = x[:, 0].view(-1, 1)\n",
    "        x2 = x[:, 1].view(-1, 1)\n",
    "        \n",
    "        x1 = self.subnet1(x1)\n",
    "        x2 = self.subnet2(x2)\n",
    "        x = (x1 + x2) / 2.0\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Net(\n  (front): FrontProcessing(\n    (in1): Linear(in_features=6, out_features=2, bias=True)\n  )\n  (subnet1): SubNet(\n    (fc1): Linear(in_features=1, out_features=10, bias=True)\n    (fc2): Linear(in_features=10, out_features=2, bias=True)\n  )\n  (subnet2): SubNet(\n    (fc1): Linear(in_features=1, out_features=10, bias=True)\n    (fc2): Linear(in_features=10, out_features=2, bias=True)\n  )\n)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "net = Net(FrontProcessing, SubNet)\n",
    "net.double()\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "X_torch = torch.DoubleTensor(X)\n",
    "Y_torch = torch.DoubleTensor(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0684, -0.2165],\n        [ 0.0400, -0.2849],\n        [ 0.2150, -0.2697],\n        ...,\n        [-0.2293, -0.1807],\n        [-0.0510, -0.1756],\n        [-0.1764, -0.1782]], dtype=torch.float64, grad_fn=<DivBackward0>)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 29
    }
   ],
   "source": [
    "net(X_torch.double())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "torch_data_set = Data.TensorDataset(X_torch, Y_torch)\n",
    "sampler = ImbalancedDatasetSampler(torch_data_set)\n",
    "loader = Data.DataLoader(dataset=torch_data_set, batch_size=10000, sampler=sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[ 0.3867,  0.2593,  0.3876, -0.0295, -0.3667, -0.1935],\n         [ 0.2780, -0.0026, -0.2029, -0.3128, -0.3821, -0.3446]],\n        dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([-0.0828,  0.2239], dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([[ 0.5406],\n         [-0.9644],\n         [ 0.6238],\n         [-0.7825],\n         [-0.2114],\n         [-0.4055],\n         [-0.1926],\n         [-0.1963],\n         [-0.8973],\n         [-0.8634]], dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([-0.1565,  0.0129, -0.4543,  0.3767, -0.9001, -0.0675,  0.8794, -0.4079,\n          0.9030,  0.3622], dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([[-0.2854,  0.2001, -0.0365, -0.1412,  0.2529, -0.2555,  0.0339, -0.0662,\n           0.2258,  0.0883],\n         [ 0.1519,  0.1117, -0.0760, -0.0665, -0.2606,  0.1713,  0.2511,  0.2164,\n          -0.2231,  0.0141]], dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([-0.2229, -0.1741], dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([[-0.5827],\n         [ 0.3417],\n         [-0.5959],\n         [-0.0218],\n         [ 0.0421],\n         [ 0.6446],\n         [-0.7559],\n         [-0.6865],\n         [-0.5807],\n         [ 0.6999]], dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([-0.3595,  0.8435,  0.3616,  0.1266, -0.0074, -0.1977,  0.1255, -0.2283,\n         -0.0070,  0.1276], dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([[-0.2474, -0.1657,  0.2554, -0.2566, -0.0227,  0.3128,  0.1142,  0.0090,\n          -0.2740,  0.1567],\n         [-0.2252, -0.0898, -0.1061, -0.0468,  0.0035,  0.2608,  0.0395,  0.2832,\n           0.1934, -0.1999]], dtype=torch.float64, requires_grad=True), Parameter containing:\n tensor([ 0.1418, -0.2235], dtype=torch.float64, requires_grad=True)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 37
    }
   ],
   "source": [
    "list(net.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch:  0 | loss:  0.5827303760328246\n##########################\n",
      "Epoch:  1 | loss:  0.5989591187351524\n##########################\n",
      "Epoch:  2 | loss:  0.6110256671136713\n##########################\n",
      "Epoch:  3 | loss:  0.6186280468063902\n##########################\n",
      "Epoch:  4 | loss:  0.6224400316661297\n##########################\n",
      "Epoch:  5 | loss:  0.6243678645755922\n##########################\n",
      "Epoch:  6 | loss:  0.6242167627464832\n##########################\n",
      "Epoch:  7 | loss:  0.6231564534524688\n##########################\n",
      "Epoch:  8 | loss:  0.6196971120763054\n##########################\n",
      "Epoch:  9 | loss:  0.6157556431852632\n##########################\n",
      "Epoch:  10 | loss:  0.6113481476553692\n##########################\n",
      "Epoch:  11 | loss:  0.6064447880078464\n##########################\n",
      "Epoch:  12 | loss:  0.6012611085124643\n##########################\n",
      "Epoch:  13 | loss:  0.5950852829790793\n##########################\n",
      "Epoch:  14 | loss:  0.5893902881640618\n##########################\n",
      "Epoch:  15 | loss:  0.5829951965636118\n##########################\n",
      "Epoch:  16 | loss:  0.5772870326918579\n##########################\n",
      "Epoch:  17 | loss:  0.5707394162023584\n##########################\n",
      "Epoch:  18 | loss:  0.5648696876856909\n##########################\n",
      "Epoch:  19 | loss:  0.5585022266727216\n##########################\n",
      "Epoch:  20 | loss:  0.552480898935713\n##########################\n",
      "Epoch:  21 | loss:  0.546387146137298\n##########################\n",
      "Epoch:  22 | loss:  0.5408895925405911\n##########################\n",
      "Epoch:  23 | loss:  0.5359049203853921\n##########################\n",
      "Epoch:  24 | loss:  0.5317085412595834\n##########################\n",
      "Epoch:  25 | loss:  0.5266120770644774\n##########################\n",
      "Epoch:  26 | loss:  0.5219609763113855\n##########################\n",
      "Epoch:  27 | loss:  0.5177147945374624\n##########################\n",
      "Epoch:  28 | loss:  0.5134323802884307\n##########################\n",
      "Epoch:  29 | loss:  0.5102390077141226\n##########################\n",
      "Epoch:  30 | loss:  0.506741970808902\n##########################\n",
      "Epoch:  31 | loss:  0.502643247103564\n##########################\n",
      "Epoch:  32 | loss:  0.4994409578872251\n##########################\n",
      "Epoch:  33 | loss:  0.4965967126031826\n##########################\n",
      "Epoch:  34 | loss:  0.4934980898982672\n##########################\n",
      "Epoch:  35 | loss:  0.4904497048644415\n##########################\n",
      "Epoch:  36 | loss:  0.4878802508972448\n##########################\n",
      "Epoch:  37 | loss:  0.4858640700886496\n##########################\n",
      "Epoch:  38 | loss:  0.4836290667950095\n##########################\n",
      "Epoch:  39 | loss:  0.4809979045499862\n##########################\n",
      "Epoch:  40 | loss:  0.4798752900023126\n##########################\n",
      "Epoch:  41 | loss:  0.4776161166939824\n##########################\n",
      "Epoch:  42 | loss:  0.4764487212709541\n##########################\n",
      "Epoch:  43 | loss:  0.4744143457409189\n##########################\n",
      "Epoch:  44 | loss:  0.47303312528125957\n##########################\n",
      "Epoch:  45 | loss:  0.4709972966641989\n##########################\n",
      "Epoch:  46 | loss:  0.4695364866081493\n##########################\n",
      "Epoch:  47 | loss:  0.46786901004319414\n##########################\n",
      "Epoch:  48 | loss:  0.4656434543142305\n##########################\n",
      "Epoch:  49 | loss:  0.46466579316941553\n##########################\n",
      "Epoch:  50 | loss:  0.46364838595759444\n##########################\n",
      "Epoch:  51 | loss:  0.46225491028272636\n##########################\n",
      "Epoch:  52 | loss:  0.4616599246979819\n##########################\n",
      "Epoch:  53 | loss:  0.46040782561766463\n##########################\n",
      "Epoch:  54 | loss:  0.4593399182617571\n##########################\n",
      "Epoch:  55 | loss:  0.4581473464330827\n##########################\n",
      "Epoch:  56 | loss:  0.4566688169831699\n##########################\n",
      "Epoch:  57 | loss:  0.4556425771631491\n##########################\n",
      "Epoch:  58 | loss:  0.4545410255458659\n##########################\n",
      "Epoch:  59 | loss:  0.4544017497449281\n##########################\n",
      "Epoch:  60 | loss:  0.45380224608493436\n##########################\n",
      "Epoch:  61 | loss:  0.4531290140221174\n##########################\n",
      "Epoch:  62 | loss:  0.4523536978150448\n##########################\n",
      "Epoch:  63 | loss:  0.4523910281095513\n##########################\n",
      "Epoch:  64 | loss:  0.45152161628247\n##########################\n",
      "Epoch:  65 | loss:  0.4504643619809888\n##########################\n",
      "Epoch:  66 | loss:  0.449570665964029\n##########################\n",
      "Epoch:  67 | loss:  0.4493923546466393\n##########################\n",
      "Epoch:  68 | loss:  0.44853105653687514\n##########################\n",
      "Epoch:  69 | loss:  0.447912050977726\n##########################\n",
      "Epoch:  70 | loss:  0.44740092313302504\n##########################\n",
      "Epoch:  71 | loss:  0.447773825914599\n##########################\n",
      "Epoch:  72 | loss:  0.44694941705183405\n##########################\n",
      "Epoch:  73 | loss:  0.4475041366837317\n##########################\n",
      "Epoch:  74 | loss:  0.44639155281380505\n##########################\n",
      "Epoch:  75 | loss:  0.44633384912998414\n##########################\n",
      "Epoch:  76 | loss:  0.44591317790214663\n##########################\n",
      "Epoch:  77 | loss:  0.44516904526185064\n##########################\n",
      "Epoch:  78 | loss:  0.44560449689361287\n##########################\n",
      "Epoch:  79 | loss:  0.44486636535369356\n##########################\n",
      "Epoch:  80 | loss:  0.44477170791433973\n##########################\n",
      "Epoch:  81 | loss:  0.4450207857283954\n##########################\n",
      "Epoch:  82 | loss:  0.44594182698365364\n##########################\n",
      "Epoch:  83 | loss:  0.4452836534389146\n##########################\n",
      "Epoch:  84 | loss:  0.44534451379523504\n##########################\n",
      "Epoch:  85 | loss:  0.44479419683632004\n##########################\n",
      "Epoch:  86 | loss:  0.444610817739823\n##########################\n",
      "Epoch:  87 | loss:  0.44433041043000604\n##########################\n",
      "Epoch:  88 | loss:  0.4436325307182986\n##########################\n",
      "Epoch:  89 | loss:  0.4430112356495456\n##########################\n",
      "Epoch:  90 | loss:  0.44291819415603023\n##########################\n",
      "Epoch:  91 | loss:  0.44274906843638906\n##########################\n",
      "Epoch:  92 | loss:  0.4426942139702569\n##########################\n",
      "Epoch:  93 | loss:  0.44269562537053025\n##########################\n",
      "Epoch:  94 | loss:  0.4430879276596637\n##########################\n",
      "Epoch:  95 | loss:  0.4431687174473022\n##########################\n",
      "Epoch:  96 | loss:  0.44292278129341356\n##########################\n",
      "Epoch:  97 | loss:  0.4425173338042556\n##########################\n",
      "Epoch:  98 | loss:  0.4426350455226824\n##########################\n",
      "Epoch:  99 | loss:  0.4426989518516752\n##########################\n",
      "Epoch:  100 | loss:  0.4416204172534906\n##########################\n",
      "Epoch:  101 | loss:  0.4415399627187212\n##########################\n",
      "Epoch:  102 | loss:  0.44118680516972364\n##########################\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d515df69fc26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "for epoch in range(600):\n",
    "    for step, (X_train, Y_train) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        X_train = X_train.requires_grad_(True)\n",
    "        \n",
    "        prediction = net(X_train)\n",
    "        loss = criterion(prediction, Y_train.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        loss_all = criterion(net(X_torch), Y_torch.long())\n",
    "        print(\"Epoch: \", epoch, \"| loss: \", loss_all.data.numpy())\n",
    "        print(\"##########################\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[ 0.6029,  0.1175, -0.0483,  0.1309, -0.4506, -0.0784],\n        [-0.1818,  0.0059,  0.5347, -0.6485, -0.1501, -0.5938]],\n       dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([-0.2021,  0.3019], dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([[ 0.6069],\n        [-0.9742],\n        [ 0.6284],\n        [-0.7815],\n        [-0.2085],\n        [-0.3799],\n        [-0.1687],\n        [-0.1953],\n        [-0.9068],\n        [-0.8715]], dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([-0.1560,  0.0284, -0.4538,  0.3816, -0.9007, -0.0953,  0.8710, -0.4086,\n         1.0051,  0.3842], dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([[-0.3596,  0.2643, -0.1116, -0.0536,  0.2528, -0.2364,  0.1515, -0.0665,\n          0.3968,  0.1800],\n        [ 0.2262,  0.0474, -0.0010, -0.1540, -0.2605,  0.1522,  0.1335,  0.2166,\n         -0.3940, -0.0776]], dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([-0.1273, -0.2697], dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([[-0.6101],\n        [ 0.3672],\n        [-0.5500],\n        [-0.0989],\n        [ 0.0376],\n        [ 0.6986],\n        [-0.7800],\n        [-0.7794],\n        [-0.7171],\n        [ 0.8566]], dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([-0.3539,  0.8749,  0.3469,  0.1175, -0.0100, -0.1668,  0.1251, -0.2064,\n         0.0241,  0.2217], dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([[-0.3567,  0.0658,  0.1090, -0.2723, -0.0134,  0.4638, -0.0642, -0.1405,\n         -0.4190,  0.3909],\n        [-0.1159, -0.3213,  0.0402, -0.0312, -0.0058,  0.1098,  0.2179,  0.4327,\n          0.3384, -0.4342]], dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor([ 0.2375, -0.3192], dtype=torch.float64, requires_grad=True)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "accuracy:79.000 %\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "out = net(X_torch)\n",
    "_, prediction = torch.max(out.data, 1)\n",
    "print('accuracy:%.3f %%' % (100 * torch.sum(Y_torch.long()==prediction) / 200000))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}